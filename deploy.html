<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pregel+</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Add custom CSS here -->
    <style>
    body {
        margin-top: 60px;
    }
    </style>

</head>

<body>

    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">Pregel+</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <li><a href="introduction.html">Introduction</a>
					</li>
					<li><a href="download.html">Download</a>
					</li>
					<li><a href="documentation.html">Documentation</a>
					</li>
					<li><a href="publications.html">Publications</a>
					</li>
					<li><a href="team.html">Team</a>
					</li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <div class="container">

        <div class="row">
            <div class="col-lg-12">
                <h1>Pregel+ Deployment in a Multi-Node Cluster</h1>
            </div>
        </div>
        <p>&nbsp;</p>
        <h2>Dependencies</h2>
        <p>
            Pregel+ is implemented in C/C++, and the current version runs in Linux. Pregel+ reads/writes data from/to files on Hadoop Distributed File System (HDFS) through libhdfs, and sends messages using MPI.
		</p>
        <p>
            We require the following dependencies.
		</p>
		
        <div class="alert alert-success">
            <ul>
                <li>G++</li>
                <li>Open MPI or MPICH</li>
                <li>JDK</li>
				<li>Hadoop</li>
            </ul>
        </div>
		The following operations are performed on each machine in the cluster.
		<p>&nbsp;</p>
        <h2>G++ Installation</h2>
		<p>If G++ is not installed on your Linux, install it first. The command depends on your Linux distribution.</p>
		<p>If you are using Ubuntu. Install G++ using the following command.</p>
		<div class="alert alert-info">
            <p>sudo apt-get install g++</p>
			<p>[type your root password]</p>
        </div>
		<p>If you are using Fedora. Install G++ using the following command.</p>
		<div class="alert alert-info">
            <p>su -c "yum install gcc-c++"</p>
			<p>[type your root password]</p>
        </div>
		<p>&nbsp;</p>
        <h2>MPI Installation</h2>
		<p>We now show how to deploy MPICH3 on each machine in the cluster. MPICH3 is highly recommended compared with other MPI versions like OpenMPI, due to its superior performance.</p>
		<p>Download MPICH3 from <a href="http://www.mpich.org/downloads/" target="_blank">mpich.org</a> and extract the contents of the MPICH package to some temporary location, before compiling the source code to binaries.</p>
		<div class="alert alert-info">
			<p>cd {the-directory-containing-downloaded-mpich-package}</p>
			<p>tar xzf mpich-3.1.tar.gz</p>
			<p>cd mpich-3.1</p>
		</div>
		<p>Choose an installation directory for MPICH3, such as <strong>/usr/local/mpich</strong>, and compile and install MPICH3.</p>
		<div class="alert alert-info">
			<p>./configure -prefix=/usr/local/mpich (--disable-f77 --disable-fc)</p>
			<p>make</p>
			<p>sudo make install <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "make install" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
		</div>
		<p>Add the following lines to the end of the file <strong>$HOME/.bashrc</strong>. Here, <b>$HOME</b> can be used interchangably with <b>/home/{username}</b> and <b>~</b>.</p>
		<div class="alert alert-warning">
			<p>export MPICH_HOME=/usr/local/mpich</p>
			<p>export PATH=$PATH:$MPICH_HOME/bin</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>.</p>
		<p>&nbsp;</p>
        <h2>JDK Installation</h2>
		<p>Hadoop requires a working Java 5+ (aka Java 1.5+) installation. We now describe the installation of OpenJDK 7.</p>
		<div class="alert alert-info">
			<p>sudo apt-get install openjdk-7-jdk <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "yum install java-1.7.0-openjdk-devel" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
		</div>
		<p>It is also necessary to add the following lines to the end of the file <b>$HOME/.bashrc</b>.</p>
		<p>[For <b>64-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</p>
			<p>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/amd64/server</p>
		</div>
		<p>[For <b>32-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386</p>
			<p>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/i386/server</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>.</p>
		<p>&nbsp;</p>
        <h2>SSH Configuration</h2>
		<p>Both Hadoop and MPI requires password-less SSH connection between the master and all the slaves. If SSH server is not installed on your Linux, install it first.</p>
		<div class="alert alert-info">
			<p>sudo apt-get install openssh-server <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "yum install openssh-server" <b>[Fedora]</b></p>
		</div>
		<p>If SSH server is not started, start it using command <b>sudo /etc/init.d/ssh start</b>.</p>		
		<p>Suppose that the cluster contains one master machine with IP "192.168.0.1" and hostname "master", and <i>N</i> slave machines where the <i>i</i>-th slave has IP "192.168.0.(<i>i</i>+1)" and hostname "slave(<i>i</i>+1)". Then, for each machine, we need to add the following lines to the end of the file <b>/etc/hosts</b>.</p>
        <div class="alert alert-warning">
			<p>192.168.0.1 master</p>
			<p>192.168.0.2 slave1</p>
			<p>192.168.0.3 slave2</p>
			<p>......</p>
			<p>192.168.0.(<i>N</i>+1) slave<i>N</i></p>
		</div>
		<p>We now show how to configure the password-less SSH connection. First, create an RSA key pair with an empty password on the master:</p>
		<div class="alert alert-info">
			<p>ssh-keygen -t rsa</p>
			<p>[Press "Enter" for all questions]</p>
		</div>
		<p>This creates files <b>id_rsa</b> and <b>id_rsa.pub</b> under the default directory <b>$HOME/.ssh/</b>. Then, we copy the public key <b>id_rsa.pub</b> to another file <b>authorized_keys</b>:</p>
		<div class="alert alert-info">
			<p>cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys</p>
		</div>
		<p>Finally, copy the public key <b>authorized_keys</b> to the directory <b>$HOME/.ssh</b> of all slaves using "scp".</p>
		<div class="alert alert-info">
			<p>scp $HOME/.ssh/authorized_keys {username}@slave1:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
			<p>scp $HOME/.ssh/authorized_keys {username}@slave2:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
			<p>......</p>
			<p>scp $HOME/.ssh/authorized_keys {username}@slave<i>N</i>:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
		</div>
		<p>Now, you should be able to connect from master to any machine using SSH, without any password.</p>
		<div class="alert alert-info">
			<p>ssh master</p>
			<p>[no password is required]</p>
			<p>exit</p>
			<p>ssh slave1</p>
			<p>[no password is required]</p>
			<p>exit</p>
		</div>
		<p>&nbsp;</p>
		<h2>Hadoop Deployment</h2>
		<p>Download <a href="http://hadoop.apache.org/releases.html" target="_blank">Hadoop</a> (a stable version like <a href="http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz" target="_blank">Hadoop 1.2.1</a> is preferred) to the master and extract the contents of the Hadoop package to <strong>/usr/local</strong>. We will copy it to the slaves using "scp" after configuration.</p>
		<div class="alert alert-info">
			<p>tar xzf hadoop-1.2.1.tar.gz</p>
			<p>sudo mv hadoop-1.2.1 /usr/local <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "mv hadoop-1.2.1 /usr/local" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
			<p>sudo chown -R {username}:{usergroup} /usr/local/hadoop-1.2.1 <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "chown -R {username}:{usergroup} /usr/local/hadoop-1.2.1" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
		</div>
		<p>In the above setting, <b>/usr/local/hadoop-1.2.1</b> is the root directory of Hadoop, which we call <b>HADOOP_HOME</b>. Make sure to add {username} as the owner of all the files in <b>HADOOP_HOME</b> (inclusive). One can find the group(s) of {username} using command <b>groups {username}</b>.</p>
		<p>It is also necessary to add the following lines to the end of the file <b>$HOME/.bashrc</b>.</p>
		<div class="alert alert-warning">
			<p>export HADOOP_HOME=/usr/local/hadoop-1.2.1</p>
			<p>export PATH=$PATH:$HADOOP_HOME/bin</p>
			<p><b>[For 64-bit Linux]</b> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/c++/Linux-amd64-64/lib</p>
			<p><b>[For 32-bit Linux]</b> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/c++/Linux-i386-32/lib</p>
		</div>
		<div class="alert alert-warning">
			<p>for i in $HADOOP_HOME/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>
			<p>for i in $HADOOP_HOME/lib/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>			
			<p>export CLASSPATH</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>. To save workload, one may edit the file <strong>$HOME/.bashrc</strong> only on the master, and use "scp" to copy it to the slaves, and then connect to each slave using "ssh" to compile the file.</p>
		<p>Next, we need to configure the files under folder <b>$HADOOP_HOME/conf</b>. Referring to the previous cluster setting again, then the content of the file <b>$HADOOP_HOME/conf/masters</b> should be updated to:</p>
		<div class="alert alert-warning">
			<p>master</p>
		</div>
		<p>The content of the file <b>$HADOOP_HOME/conf/slaves</b> should be updated to:</p>
		<div class="alert alert-warning">
			<p>slave1</p>
			<p>slave2</p>
			<p>......</p>
			<p>slave<i>N</i></p>
		</div>
		<p>Add the <b>JAVA_HOME</b> environment variable to the end of the file <b>$HADOOP_HOME/conf/hadoop-env.sh</b>.</p>
		<p>[For <b>64-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</p>
		</div>
		<p>[For <b>32-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386</p>
		</div>
		<p>Create a directory for holding the data of HDFS (Hadoop Distributed File System) such as <b>/app/hdfs</b>, and set the required ownership and permissions for {username}.</p>
		<div class="alert alert-info">
			<p>sudo mkdir -p /app/hdfs <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "mkdir -p /app/hdfs" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
			<p>sudo chown {username}:{usergroup} /app/hdfs <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "chown {username}:{usergroup} /app/hdfs" <b>[Fedora]</b></p>
			<p>sudo chmod 755 /app/hdfs <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "chmod 755 /app/hdfs" <b>[Fedora]</b></p>
		</div>
		<p>Add the following properties to the file <b>$HADOOP_HOME/conf/core-site.xml</b>:</p>
		<div class="alert alert-warning">
			<p>&lt;property&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;/app/hdfs&lt;/value&gt;</p>
			<p>&lt;/property&gt;</p>
			<p>&lt;property&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;fs.default.name&lt;/name&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;hdfs://master:9000&lt;/value&gt;</p>
			<p>&lt;/property&gt;</p>
		</div>
		<p>This indicates that HDFS data are stored under <b>/app/hdfs</b>, and the HDFS master (aka NameNode) resides at <b>master</b> with port <b>9000</b>.</p>
		<p>Add the following property to the file <b>$HADOOP_HOME/conf/mapred-site.xml</b>:</p>
		<div class="alert alert-warning">
			<p>&lt;property&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;mapred.job.tracker&lt;/name&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;hdfs://master:9001&lt;/value&gt;</p>
			<p>&lt;/property&gt;</p>
		</div>
		<p>This specifies the location of JobTracker that manages MapReduce/Giraph jobs.</p>
		<p>Add the following property to the file <b>$HADOOP_HOME/conf/hdfs-site.xml</b>:</p>
		<div class="alert alert-warning">
			<p>&lt;property&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;dfs.replication&lt;/name&gt;</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;3&lt;/value&gt;</p>
			<p>&lt;/property&gt;</p>
		</div>
		<p>This configuration asks HDFS to replicate any data three times, so that data won't be lost even if a machine is down.</p>
		<p>Other properties of Hadoop can also be configured in the files according to the your need. Now that Hadoop is properly configured on the master, we use "scp" to copy it to slaves. For each slave<i>i</i>, we do the following operations on the master:</p>
		<p>(Commands below are for Ubuntu, use <b>su -c "command"</b> for Fedora instead)</p>
		<div class="alert alert-info">
			<p>sudo scp -r /usr/local/hadoop-1.2.1 {username}@slave<i>i</i>:$HOME</p>
			<p>[type your root password]</p>
			<p>ssh slave<i>i</i></p>
			<p>sudo mv hadoop-1.2.1 /usr/local/</p>
			<p>[type your root password]</p>
			<p>sudo chown -R {username}:{usergroup} /usr/local/hadoop-1.2.1</p>
			<p>sudo mkdir -p /app/hdfs</p>
			<p>sudo chown {username}:{usergroup} /app/hdfs</p>
			<p>sudo chmod 755 /app/hdfs</p>
			<p>exit</p>
		</div>
		<p>Before we start the newly configured Hadoop cluster, we must format HDFS via the NameNode. Type the following command on <b>master</b> to format HDFS:
		<div class="alert alert-info">
			<p>hadoop namenode -format</p>
		</div>
		<div class="alert alert-danger">
			<p>Warning: formatting an existing Hadoop cluster erases all data in HDFS!<p>
		</div>
		<p>Now, you are ready to start the Hadoop cluster. To start both HDFS and JobTracker, type the following command on <b>master</b>:</p>
		<div class="alert alert-info">
			<p>$HADOOP_HOME/bin/start-all.sh<p>
		</div>
		<p>To see whether the Hadoop cluster is properly started, you may use the "jps" command. On the master, you should see processes "Jps", "SecondaryNameNode", "JobTracker", and "NameNode". On a slave, you should see "DataNode", "Jps", and "TaskTracker". You are now able to access http://master:50070 to view the state of HDFS, and access http://master:50030 to view the state of MapReduce/Giraph jobs.</p>
		<p>To stop the Hadoop cluster, type the following command on <b>master</b>:</p>
		<div class="alert alert-info">
			<p>$HADOOP_HOME/bin/stop-all.sh<p>
		</div>
		<p>If you are not using Hadoop MapReduce, or Giraph, but only Pregel+, there is no need to start the JobTracker. You only need to start HDFS using the following command:</p>
		<div class="alert alert-info">
			<p>$HADOOP_HOME/bin/start-dfs.sh<p>
		</div>
		<p>You may stop HDFS later on using the following command:</p>
		<div class="alert alert-info">
			<p>$HADOOP_HOME/bin/stop-dfs.sh<p>
		</div>
	</div>
    <!-- /.container -->

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>

</body>

</html>
