<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pregel+</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Add custom CSS here -->
    <style>
    body {
        margin-top: 60px;
    }
    </style>

</head>

<body>

    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html">Pregel+</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <li><a href="download.html">Download</a>
					</li>
					<li><a href="documentation.html">Documentation</a>
					</li>
					<li><a href="publications.html">Publications</a>
					</li>
					<li><a href="team.html">Team</a>
					</li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <div class="container">

        <div class="row">
            <div class="col-lg-12">
                <h1>Pregel+ Deployment in a Multi-Node Cluster</h1>
            </div>
        </div>
        <p>&nbsp;</p>
        <h2>Dependencies</h2>
        <p>
            Pregel+ is implemented in C/C++, and the current version runs in Linux. Pregel+ reads/writes data from/to files on Hadoop Distributed File System (HDFS) through libhdfs, and sends messages using MPI.
		</p>
        <p>
            We require the following dependencies.
		</p>
		
        <div class="alert alert-success">
            <ul>
                <li>G++</li>
                <li>Open MPI or MPICH</li>
                <li>JDK</li>
				<li>Hadoop</li>
            </ul>
        </div>
		The following operations are performed on each machine in the cluster.
		<p>&nbsp;</p>
        <h2>G++ Installation</h2>
		<p>If G++ is not installed on your Linux, install it first. The command depends on your Linux distribution.</p>
		<p>If you are using Ubuntu. Install G++ using the following command.</p>
		<div class="alert alert-info">
            <p>sudo apt-get install g++</p>
			<p>[type your root password]</p>
        </div>
		<p>If you are using Fedora. Install G++ using the following command.</p>
		<div class="alert alert-info">
            <p>su -c "yum install gcc-c++"</p>
			<p>[type your root password]</p>
        </div>
		<p>&nbsp;</p>
        <h2>MPI Installation</h2>
		<p>We now show how to deploy MPICH3 on <strong>each</strong> machine in the cluster. MPICH3 is highly recommended compared with other MPI versions like OpenMPI, due to its superior performance.</p>
		<p>Download MPICH3 from <a href="http://www.mpich.org/downloads/" target="_blank">mpich.org</a> and extract the contents of the MPICH package to some temporary location, before compiling the source code to binaries.</p>
		<div class="alert alert-info">
			<p>cd {the-directory-containing-downloaded-mpich-package}</p>
			<p>tar xzf mpich-3.1.tar.gz</p>
			<p>cd mpich-3.1</p>
		</div>
		<p>Choose an installation directory for MPICH3, such as <strong>/usr/local/mpich</strong>, and compile and install MPICH3.</p>
		<div class="alert alert-info">
			<p>./configure -prefix=/usr/local/mpich (--disable-f77 --disable-fc)</p>
			<p>make</p>
			<p>sudo make install <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "make install" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
		</div>
		<p>Add the following lines to the end of the file <strong>$HOME/.bashrc</strong>. Here, <b>$HOME</b> can be used interchangably with <b>/home/{username}</b> and <b>~</b>.</p>
		<div class="alert alert-warning">
			<p>export MPICH_HOME=/usr/local/mpich</p>
			<p>export PATH=$PATH:$MPICH_HOME/bin</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>.</p>
		<p>&nbsp;</p>
        <h2>JDK Installation</h2>
		<p>Hadoop requires a working Java 5+ (aka Java 1.5+) installation. We now describe the installation of OpenJDK 7.</p>
		<div class="alert alert-info">
			<p>sudo apt-get install openjdk-7-jdk <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "yum install java-1.7.0-openjdk-devel" <b>[Fedora]</b></p>
			<p>[type your root password]</p>
		</div>
		<p>It is also necessary to add the following lines to the end of the file <b>$HOME/.bashrc</b>.</p>
		<p>[For <b>64-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</p>
			<p>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/amd64/server</p>
		</div>
		<p>[For <b>32-bit</b> Linux]</p>
		<div class="alert alert-warning">
			<p>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386</p>
			<p>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/i386/server</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>.</p>
		<p>&nbsp;</p>
        <h2>SSH Configuration</h2>
		<p>Both Hadoop and MPI requires password-less SSH connection between the master and all the slaves. If SSH server is not installed on your Linux, install it first.</p>
		<div class="alert alert-info">
			<p>sudo apt-get install openssh-server <b>[Ubuntu]</b>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;su -c "yum install openssh-server" <b>[Fedora]</b></p>
		</div>
		<p>If SSH server is not started, start it using command <b>sudo /etc/init.d/ssh start</b>.</p>		
		<p>Suppose that the cluster contains one master machine with IP "192.168.0.1" and hostname "master", and <i>N</i> slave machines where the <i>i</i>-th slave has IP "192.168.0.(<i>i</i>+1)" and hostname "slave(<i>i</i>+1)". Then, for each machine, we need to add the following lines to the end of the file <b>/etc/hosts</b>.</p>
        <div class="alert alert-warning">
			<p>192.168.0.1 master</p>
			<p>192.168.0.2 slave1</p>
			<p>192.168.0.3 slave2</p>
			<p>......</p>
			<p>192.168.0.(<i>N</i>+1) slave<i>N</i></p>
		</div>
		<p>We now show how to configure the password-less SSH connection. First, create an RSA key pair with an empty password on the master:</p>
		<div class="alert alert-info">
			<p>ssh-keygen -t rsa</p>
			<p>[Press "Enter" for all questions]</p>
		</div>
		<p>This creates files <b>id_rsa</b> and <b>id_rsa.pub</b> under the default directory <b>$HOME/.ssh/</b>. Then, we copy the public key <b>id_rsa.pub</b> to another file <b>authorized_keys</b>:</p>
		<div class="alert alert-info">
			<p>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys</p>
		</div>
		<p>Finally, copy the public key <b>authorized_keys</b> to the directory <b>$HOME/.ssh</b> of all slaves using "scp".</p>
		<div class="alert alert-info">
			<p>scp $HOME/.ssh/authorized_keys {username}@slave1:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
			<p>scp $HOME/.ssh/authorized_keys {username}@slave2:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
			<p>......</p>
			<p>scp $HOME/.ssh/authorized_keys {username}@slave<i>N</i>:$HOME/.ssh/</p>
			<p>[type {username}'s password]</p>
		</div>
		<p>Now, you should be able to connect from master to any machine using SSH, without any password.</p>
		<div class="alert alert-info">
			<p>ssh master</p>
			<p>[no password is required]</p>
			<p>exit</p>
			<p>ssh slave1</p>
			<p>[no password is required]</p>
			<p>exit</p>
		</div>
		<p>&nbsp;</p>
		<h2>Hadoop Deployment</h2>
        <p>We assume that you've already install Hadoop 2.x correctly. Please also make sure the CLASSPATH is also
        configured correctly. If you are not sure, please append the following in your ~/.bashrc(assuming you are using
        bash)</p>
		<div class="alert alert-warning">
			<p>for i in $HADOOP_HOME/share/hadoop/hdfs/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>
			<p>for i in $HADOOP_HOME/share/hadoop/hdfs/lib/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>
			<p>for i in $HADOOP_HOME/share/hadoop/common/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>
			<p>for i in $HADOOP_HOME/share/hadoop/common/lib/*.jar</p>
			<p>do</p>
			<p>&nbsp;&nbsp;&nbsp;&nbsp;CLASSPATH=$CLASSPATH:$i</p>
			<p>done</p>
			<p>export CLASSPATH</p>
		</div>
		<p>Compile the file with the command <strong>source $HOME/.bashrc</strong>. To save workload, one may edit the file <strong>$HOME/.bashrc</strong> only on the master, and use "scp" to copy it to the slaves, and then connect to each slave using "ssh" to compile the file.</p>
	</div>
    <!-- /.container -->

    <!-- JavaScript -->
    <script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create', 'UA-53484769-1', 'auto');ga('send', 'pageview');</script>
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
    <footer class="small">
    <hr>
    <br>
    &nbsp;
    <br>
    &nbsp;
    <br>
    &nbsp;
    <br>
    &nbsp;
    <br>
    </footer>
</body>

</html>
